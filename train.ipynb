{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "57ecf403b3c00f8d1e64b0af6aa3b510656975aa9bca45de25eb3516a12eb651"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/chaofeng/miniconda3/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from transformers import BartTokenizer, BartModel\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from pytorch_lightning import Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Huggingface transformers\n",
    "import transformers\n",
    "from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "#handling html data\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/chaofeng/Documents/vscode/finsim/data/kg'\n",
    "trainfile = base_path + \"/train.json\"\n",
    "testfile = base_path + '/test.json'\n",
    "raw_datasets = pd.read_json(trainfile)\n",
    "test_datasets = pd.read_json(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = LabelBinarizer()\n",
    "y = raw_datasets.label.tolist()\n",
    "yt = mlb.fit_transform(y)\n",
    "x = raw_datasets.term.tolist()\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, yt , test_size=0.1, random_state=42,shuffle=True)\n",
    "\n",
    "x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42,shuffle=True)\n",
    "testset = test_datasets.term.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinsimDataset (Dataset):\n",
    "    def __init__(self,quest,tags, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = quest\n",
    "        self.labels = tags\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        text = self.text[item_idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True, # Add [CLS] [SEP]\n",
    "            max_length= self.max_len,\n",
    "            padding = 'max_length',\n",
    "            return_token_type_ids= False,\n",
    "            return_attention_mask= True, # Differentiates padded vs normal token\n",
    "            truncation=True, # Truncate data beyond max length\n",
    "            return_tensors = 'pt' # PyTorch Tensor format\n",
    "          )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].flatten()\n",
    "        attn_mask = inputs['attention_mask'].flatten()\n",
    "        #token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids ,\n",
    "            'attention_mask': attn_mask,\n",
    "            'label': torch.tensor(self.labels[item_idx], dtype=torch.float)\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinsimDataModule (pl.LightningDataModule):    \n",
    "    def __init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,tokenizer,batch_size=16,max_token_len=200):\n",
    "        super().__init__()\n",
    "        self.tr_text = x_tr\n",
    "        self.tr_label = y_tr\n",
    "        self.val_text = x_val\n",
    "        self.val_label = y_val\n",
    "        self.test_text = x_test\n",
    "        self.test_label = y_test\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = FinsimDataset(quest=self.tr_text,  tags=self.tr_label,tokenizer=self.tokenizer,max_len= self.max_token_len)\n",
    "        self.val_dataset= FinsimDataset(quest=self.val_text, tags=self.val_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\n",
    "        self.test_dataset =FinsimDataset(quest=self.test_text, tags=self.test_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\n",
    "        \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,batch_size= self.batch_size, shuffle = True , num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader (self.val_dataset,batch_size= 16)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader (self.test_dataset,batch_size= 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BERT_MODEL_NAME = \"ProsusAI/finbert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 100\n",
    "LR = 2e-04\n",
    "Bert_tokenizer = tokenizer\n",
    "datamodule = FinsimDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,Bert_tokenizer,BATCH_SIZE,MAX_LEN)\n",
    "datamodule.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinsimClassifier(pl.LightningModule):\n",
    "    # Set up the classifier\n",
    "    def __init__(self,n_classes=17,steps_per_epoch=None,n_epochs=3, lr=2e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert=BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "        # self.bert = BartModel.from_pretrained('facebook/bart-large', return_dict=True)\n",
    "        # self.bilstm = nn.LSTM(self.bert.config.hidden_size, 256, bidirectional=True)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self,input_ids, attn_mask):\n",
    "        output = self.bert(input_ids=input_ids,attention_mask=attn_mask)   \n",
    "        # output, _ = self.bilstm(output.last_hidden_state)\n",
    "        # avg_pool = torch.mean(output, 1)\n",
    "        output = self.classifier(output.pooler_output)    \n",
    "        return output\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(input_ids,attention_mask)\n",
    "        loss = self.criterion(outputs,labels)\n",
    "        self.log('train_loss',loss , prog_bar=True,logger=True)\n",
    "        \n",
    "        return {\"loss\" :loss, \"predictions\":outputs, \"labels\": labels }\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(input_ids,attention_mask)\n",
    "        loss = self.criterion(outputs,labels)\n",
    "        self.log('val_loss',loss , prog_bar=True,logger=True)        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(input_ids,attention_mask)\n",
    "        loss = self.criterion(outputs,labels)\n",
    "        self.log('test_loss',loss , prog_bar=True,logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters() , lr=self.lr)\n",
    "        warmup_steps = self.steps_per_epoch//3\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert       | BertModel         | 109 M \n",
      "1 | classifier | Linear            | 13.1 K\n",
      "2 | criterion  | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.981   Total estimated model params size (MB)\n",
      "Epoch 0:   0%|          | 0/36 [00:00<?, ?it/s] /home/chaofeng/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  83%|████████▎ | 30/36 [00:04<00:00,  6.63it/s]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 36/36 [00:04<00:00,  7.40it/s, loss=0.754, v_num=73, val_loss=0.756, train_loss=0.751]\n",
      "                                                           \u001b[A/home/chaofeng/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "Epoch 1:  83%|████████▎ | 30/36 [00:04<00:00,  6.72it/s, loss=0.754, v_num=73, val_loss=0.756, train_loss=0.751]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 36/36 [00:04<00:00,  7.50it/s, loss=0.507, v_num=73, val_loss=0.358, train_loss=0.377]\n",
      "Epoch 2:  83%|████████▎ | 30/36 [00:04<00:00,  6.73it/s, loss=0.507, v_num=73, val_loss=0.358, train_loss=0.377]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 36/36 [00:04<00:00,  7.52it/s, loss=0.252, v_num=73, val_loss=0.214, train_loss=0.222]\n",
      "Epoch 3:  83%|████████▎ | 30/36 [00:04<00:00,  6.74it/s, loss=0.252, v_num=73, val_loss=0.214, train_loss=0.222]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 36/36 [00:04<00:00,  7.53it/s, loss=0.185, v_num=73, val_loss=0.175, train_loss=0.169]\n",
      "Epoch 4:  83%|████████▎ | 30/36 [00:04<00:00,  6.72it/s, loss=0.185, v_num=73, val_loss=0.175, train_loss=0.169]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 36/36 [00:04<00:00,  7.50it/s, loss=0.15, v_num=73, val_loss=0.145, train_loss=0.142] \n",
      "Epoch 5:  83%|████████▎ | 30/36 [00:04<00:00,  6.71it/s, loss=0.15, v_num=73, val_loss=0.145, train_loss=0.142]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 36/36 [00:04<00:00,  7.51it/s, loss=0.117, v_num=73, val_loss=0.117, train_loss=0.092]\n",
      "Epoch 6:  83%|████████▎ | 30/36 [00:04<00:00,  6.72it/s, loss=0.117, v_num=73, val_loss=0.117, train_loss=0.092]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 36/36 [00:04<00:00,  7.51it/s, loss=0.0906, v_num=73, val_loss=0.0961, train_loss=0.0709]\n",
      "Epoch 7:  83%|████████▎ | 30/36 [00:04<00:00,  6.70it/s, loss=0.0906, v_num=73, val_loss=0.0961, train_loss=0.0709]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 36/36 [00:04<00:00,  7.49it/s, loss=0.0675, v_num=73, val_loss=0.0793, train_loss=0.0441]\n",
      "Epoch 8:  83%|████████▎ | 30/36 [00:04<00:00,  6.68it/s, loss=0.0675, v_num=73, val_loss=0.0793, train_loss=0.0441]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 36/36 [00:04<00:00,  7.46it/s, loss=0.0585, v_num=73, val_loss=0.0718, train_loss=0.0897]\n",
      "Epoch 9:  83%|████████▎ | 30/36 [00:04<00:00,  6.74it/s, loss=0.0585, v_num=73, val_loss=0.0718, train_loss=0.0897]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 36/36 [00:04<00:00,  7.53it/s, loss=0.0483, v_num=73, val_loss=0.0591, train_loss=0.0269]\n",
      "Epoch 10:  83%|████████▎ | 30/36 [00:04<00:00,  6.68it/s, loss=0.0483, v_num=73, val_loss=0.0591, train_loss=0.0269]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 36/36 [00:04<00:00,  7.46it/s, loss=0.0373, v_num=73, val_loss=0.0517, train_loss=0.0356]\n",
      "Epoch 11:  83%|████████▎ | 30/36 [00:04<00:00,  6.68it/s, loss=0.0373, v_num=73, val_loss=0.0517, train_loss=0.0356]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 36/36 [00:04<00:00,  7.47it/s, loss=0.0289, v_num=73, val_loss=0.0436, train_loss=0.0364]\n",
      "Epoch 12:  83%|████████▎ | 30/36 [00:04<00:00,  6.68it/s, loss=0.0289, v_num=73, val_loss=0.0436, train_loss=0.0364]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 36/36 [00:04<00:00,  7.46it/s, loss=0.0216, v_num=73, val_loss=0.0436, train_loss=0.012] \n",
      "Epoch 13:  83%|████████▎ | 30/36 [00:04<00:00,  6.68it/s, loss=0.0216, v_num=73, val_loss=0.0436, train_loss=0.012]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 36/36 [00:04<00:00,  7.46it/s, loss=0.019, v_num=73, val_loss=0.0423, train_loss=0.020] \n",
      "Epoch 14:  83%|████████▎ | 30/36 [00:04<00:00,  6.71it/s, loss=0.019, v_num=73, val_loss=0.0423, train_loss=0.020]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 36/36 [00:04<00:00,  7.49it/s, loss=0.0156, v_num=73, val_loss=0.0408, train_loss=0.0133]\n",
      "Epoch 15:  83%|████████▎ | 30/36 [00:04<00:00,  6.61it/s, loss=0.0156, v_num=73, val_loss=0.0408, train_loss=0.0133]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 36/36 [00:04<00:00,  7.39it/s, loss=0.0142, v_num=73, val_loss=0.0369, train_loss=0.0129]\n",
      "Epoch 16:  83%|████████▎ | 30/36 [00:04<00:00,  6.68it/s, loss=0.0142, v_num=73, val_loss=0.0369, train_loss=0.0129]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 36/36 [00:04<00:00,  7.47it/s, loss=0.0148, v_num=73, val_loss=0.0427, train_loss=0.0103]\n",
      "Epoch 17:  83%|████████▎ | 30/36 [00:04<00:00,  6.74it/s, loss=0.0148, v_num=73, val_loss=0.0427, train_loss=0.0103]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 36/36 [00:04<00:00,  7.52it/s, loss=0.0193, v_num=73, val_loss=0.044, train_loss=0.0349] \n",
      "Epoch 18:  83%|████████▎ | 30/36 [00:04<00:00,  6.73it/s, loss=0.0193, v_num=73, val_loss=0.044, train_loss=0.0349]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 36/36 [00:04<00:00,  7.52it/s, loss=0.0176, v_num=73, val_loss=0.0542, train_loss=0.00943]\n",
      "Epoch 19:  83%|████████▎ | 30/36 [00:04<00:00,  6.74it/s, loss=0.0176, v_num=73, val_loss=0.0542, train_loss=0.00943]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 36/36 [00:04<00:00,  7.53it/s, loss=0.0271, v_num=73, val_loss=0.0488, train_loss=0.0207] \n",
      "Epoch 19: 100%|██████████| 36/36 [00:04<00:00,  7.52it/s, loss=0.0271, v_num=73, val_loss=0.0488, train_loss=0.0207]\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(x_tr)//BATCH_SIZE\n",
    "model = FinsimClassifier(n_classes=17, steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',# monitored quantity\n",
    "    filename='QTag-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3, #  save the top 3 models\n",
    "    mode='min', # mode of the monitored quantity  for optimization\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback],progress_bar_refresh_rate = 30)\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/chaofeng/Documents/vscode/Finsim-1/models/lightning_logs/version_73/checkpoints/QTag-epoch=15-val_loss=0.04.ckpt\n",
      "Testing: 0it [00:00, ?it/s]/home/chaofeng/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|██████████| 7/7 [00:00<00:00, 37.02it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.04577893391251564}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.04577893391251564}]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model_path = checkpoint_callback.best_model_path\n",
    "print(model_path)\n",
    "\n",
    "trainer.test(model,datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/chaofeng/Documents/vscode/Finsim-1/models/lightning_logs/version_73/checkpoints/QTag-epoch=15-val_loss=0.04.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 9228), started 10:15:49 ago. (Use '!kill 9228' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/chaofeng/Documents/vscode/Finsim-1/models/lightning_logs/version_73/checkpoints/QTag-epoch=15-val_loss=0.04.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model_path = checkpoint_callback.best_model_path\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(105, 105)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(y_test), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Tokenize all questions in x_test\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "for quest in x_test:\n",
    "    encoded_quest =  Bert_tokenizer.encode_plus(\n",
    "                    quest,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length= MAX_LEN,\n",
    "                    padding = 'max_length',\n",
    "                    return_token_type_ids= False,\n",
    "                    return_attention_mask= True,\n",
    "                    truncation=True,\n",
    "                    return_tensors = 'pt'      \n",
    "    )\n",
    "    \n",
    "    # Add the input_ids from encoded question to the list.    \n",
    "    input_ids.append(encoded_quest['input_ids'])\n",
    "    # Add its attention mask \n",
    "    attention_masks.append(encoded_quest['attention_mask'])\n",
    "    \n",
    "# Now convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_test)\n",
    "\n",
    "# Set the batch size.  \n",
    "TEST_BATCH_SIZE = 64  \n",
    "\n",
    "# Create the DataLoader.\n",
    "pred_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "pred_sampler = SequentialSampler(pred_data)\n",
    "pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_pred_outs = 0\n",
    "flat_true_labels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FinsimClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model = model.to('cuda') # moving model to cuda\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outs, true_labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in pred_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_attn_mask, b_labels = batch\n",
    " \n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        pred_out = model(b_input_ids,b_attn_mask)\n",
    "        pred_out = torch.sigmoid(pred_out)\n",
    "        # Move predicted output and labels to CPU\n",
    "        pred_out = pred_out.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        #i+=1\n",
    "        # Store predictions and true labels\n",
    "        #print(i)\n",
    "        #print(outputs)\n",
    "        #print(logits)\n",
    "        #print(label_ids)\n",
    "    pred_outs.append(pred_out)\n",
    "    true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00229858, 0.00220319, 0.00157888, 0.00239198, 0.00238328,\n",
       "       0.991137  , 0.00234041, 0.00328559, 0.00288465, 0.00237044,\n",
       "       0.00283643, 0.0017684 , 0.0039663 , 0.00224881, 0.00256941,\n",
       "       0.00202786, 0.00247595], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "pred_outs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_pred_outs = np.concatenate(pred_outs, axis=0)\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((105, 17), (105, 17))"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "flat_pred_outs.shape , flat_true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "#define candidate threshold values\n",
    "threshold  = np.arange(0.4,0.51,0.01)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(pred_prob,thresh):\n",
    "    y_pred = []\n",
    "\n",
    "    for tag_label_row in pred_prob:\n",
    "        temp=[]\n",
    "        for tag_label in tag_label_row:\n",
    "            if tag_label >= thresh:\n",
    "                temp.append(1) # Infer tag value as 1 (present)\n",
    "            else:\n",
    "                temp.append(0) # Infer tag value as 0 (absent)\n",
    "        y_pred.append(temp)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00232111, 0.00217112, 0.00156825, 0.00239126, 0.0023779 ,\n",
       "       0.99104965, 0.0023092 , 0.00334024, 0.00282741, 0.00233517,\n",
       "       0.00280103, 0.00175893, 0.00385609, 0.00221766, 0.00252874,\n",
       "       0.00202016, 0.00252985], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "flat_pred_outs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "flat_true_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "scores=[] # Store the list of f1 scores for prediction on each threshold\n",
    "\n",
    "#convert labels to 1D array\n",
    "y_true = flat_true_labels.ravel() \n",
    "\n",
    "for thresh in threshold:\n",
    "    \n",
    "    #classes for each threshold\n",
    "    pred_bin_label = classify(flat_pred_outs,thresh) \n",
    "\n",
    "    #convert to 1D array\n",
    "    y_pred = np.array(pred_bin_label).ravel()\n",
    "\n",
    "    scores.append(metrics.f1_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal Threshold Value = 0.4800000000000001\n"
     ]
    }
   ],
   "source": [
    "opt_thresh = threshold[scores.index(max(scores))]\n",
    "print(f'Optimal Threshold Value = {opt_thresh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions for optimal threshold\n",
    "y_pred_labels = classify(flat_pred_outs,opt_thresh)\n",
    "y_pred = np.array(y_pred_labels).ravel() # Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1680\n           1       0.95      0.88      0.91       105\n\n    accuracy                           0.99      1785\n   macro avg       0.97      0.94      0.95      1785\nweighted avg       0.99      0.99      0.99      1785\n\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlb.inverse_transform(np.array(y_pred_labels))\n",
    "y_act = mlb.inverse_transform(flat_true_labels)\n",
    "\n",
    "df = pd.DataFrame({'Body':x_test,'Actual Tags':y_act,'Predicted Tags':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 Body  \\\n",
       "76                              S&P Global Inc. US-NY   \n",
       "71                               Caja de Valores S.A.   \n",
       "83                    Dow Jones Asian Titans 50 Index   \n",
       "10                       Latin America Corporate Bond   \n",
       "97                           North American Corporate   \n",
       "51              MSCI EMU IMI U.S. Dollar Hedged Index   \n",
       "16                      Andorran Financial Authority    \n",
       "33        Barbados Central Securities Depository Inc.   \n",
       "67  Herausgebergemeinschaft Wertpapier-Mitteilunge...   \n",
       "79              STOXX Europe Select Dividend 30 Index   \n",
       "\n",
       "                      Actual Tags                 Predicted Tags  \n",
       "76              Stock Corporation              Stock Corporation  \n",
       "71  Central Securities Depository  Central Securities Depository  \n",
       "83                   Equity Index                   Equity Index  \n",
       "10                   Credit Index                          Bonds  \n",
       "97                   Credit Index                   Credit Index  \n",
       "51                   Equity Index                   Equity Index  \n",
       "16              Regulatory Agency              Regulatory Agency  \n",
       "33  Central Securities Depository  Central Securities Depository  \n",
       "67              Stock Corporation                          Bonds  \n",
       "79                   Equity Index                   Equity Index  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Body</th>\n      <th>Actual Tags</th>\n      <th>Predicted Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>76</th>\n      <td>S&amp;P Global Inc. US-NY</td>\n      <td>Stock Corporation</td>\n      <td>Stock Corporation</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>Caja de Valores S.A.</td>\n      <td>Central Securities Depository</td>\n      <td>Central Securities Depository</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>Dow Jones Asian Titans 50 Index</td>\n      <td>Equity Index</td>\n      <td>Equity Index</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Latin America Corporate Bond</td>\n      <td>Credit Index</td>\n      <td>Bonds</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>North American Corporate</td>\n      <td>Credit Index</td>\n      <td>Credit Index</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>MSCI EMU IMI U.S. Dollar Hedged Index</td>\n      <td>Equity Index</td>\n      <td>Equity Index</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Andorran Financial Authority</td>\n      <td>Regulatory Agency</td>\n      <td>Regulatory Agency</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Barbados Central Securities Depository Inc.</td>\n      <td>Central Securities Depository</td>\n      <td>Central Securities Depository</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>Herausgebergemeinschaft Wertpapier-Mitteilunge...</td>\n      <td>Stock Corporation</td>\n      <td>Bonds</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>STOXX Europe Select Dividend 30 Index</td>\n      <td>Equity Index</td>\n      <td>Equity Index</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FinsimClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "QTmodel = FinsimClassifier.load_from_checkpoint(model_path)\n",
    "QTmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question):\n",
    "    text_enc = Bert_tokenizer.encode_plus(\n",
    "            question,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length= MAX_LEN,\n",
    "            padding = 'max_length',\n",
    "            return_token_type_ids= False,\n",
    "            return_attention_mask= True,\n",
    "            truncation=True,\n",
    "            return_tensors = 'pt'      \n",
    "    )\n",
    "    outputs = QTmodel(text_enc['input_ids'], text_enc['attention_mask'])\n",
    "    pred_out = outputs[0].detach().numpy()\n",
    "    #print(f'Outputs = {outputs}')\n",
    "    #print(f'Type = {type(outputs)}')\n",
    "    #print(f'Pred Outputs = {pred_out}')\n",
    "    #print(f'Type = {type(pred_out)}')\n",
    "    #preds = np.round(pred_out)\n",
    "    preds = [(pred > opt_thresh) for pred in pred_out ]\n",
    "    #pred_list = [ round(pred) for pred in pred_logits ]\n",
    "    preds = np.asarray(preds)\n",
    "    #print(f'Predictions = {preds}')\n",
    "    #print(f'Type = {type(preds)}')\n",
    "    #print(mlb.classes_)\n",
    "    new_preds = preds.reshape(1,-1).astype(int)\n",
    "    #print(new_preds)\n",
    "    pred_tags = mlb.inverse_transform(new_preds)\n",
    "    #print(mlb.inverse_transform(np.array(new_preds)))\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f64398414546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/finbert1.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "par = tqdm.tqdm(total=len(testset), ncols=100)\n",
    "with open(base_path+'/finbert1.json', 'w') as f:    \n",
    "    for i, term in enumerate(testset):\n",
    "        output = {}\n",
    "        par.update(1)\n",
    "        output['term'] = term\n",
    "        # output['label'] = y[i]\n",
    "        output['predicted_labels'] = [predict(term)[0]]\n",
    "        outputs.append(output)\n",
    "    json.dump(outputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1050/1050 [02:41<00:00,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "par.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset"
   ]
  }
 ]
}